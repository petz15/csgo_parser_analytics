{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf6e1f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries - Enhanced for Statistical Analysis\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Set Up Database Connection\n",
    "# Replace the placeholders with your actual database credentials\n",
    "DB_CONFIG = {\n",
    "    \"dbname\": \"csgo_parsed\",\n",
    "    \"user\": \"csgo_parser\",\n",
    "    \"password\": \"3?6B7yTGPrkJF34p\",\n",
    "    \"host\": \"192.168.1.100\",\n",
    "    \"port\": \"5444\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96d0a003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database connection established\n",
      "üìä Helper functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Ensure we have a fresh connection\n",
    "try:\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    print(\"‚úÖ Database connection established\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection error: {e}\")\n",
    "\n",
    "def get_descriptive_stats(data, column_name):\n",
    "    \"\"\"Calculate comprehensive descriptive statistics\"\"\"\n",
    "    stats_dict = {\n",
    "        'count': len(data),\n",
    "        'min': data.min(),\n",
    "        'max': data.max(),\n",
    "        'mean': data.mean(),\n",
    "        'median': data.median(),\n",
    "        'std': data.std(),\n",
    "        'q25': data.quantile(0.25),\n",
    "        'q75': data.quantile(0.75)\n",
    "    }\n",
    "    return stats_dict\n",
    "\n",
    "def get_top_values(data, n=15):\n",
    "    \"\"\"Get top N occurring values with percentages\"\"\"\n",
    "    value_counts = data.value_counts().head(n)\n",
    "    percentages = (value_counts / len(data) * 100).round(2)\n",
    "    return pd.DataFrame({\n",
    "        'value': value_counts.index,\n",
    "        'count': value_counts.values,\n",
    "        'percentage': percentages.values\n",
    "    })\n",
    "\n",
    "def create_distribution_plots(data, title, bins=50):\n",
    "    \"\"\"Create histogram and box plot for a variable\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    ax1.hist(data.dropna(), bins=bins, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_title(f'{title} - Distribution')\n",
    "    ax1.set_xlabel('Value')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot\n",
    "    ax2.boxplot(data.dropna())\n",
    "    ax2.set_title(f'{title} - Box Plot')\n",
    "    ax2.set_ylabel('Value')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"üìä Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3437ca30",
   "metadata": {},
   "source": [
    "# CS:GO Economy Agent-Based Model (ABM) - Data Analysis v2\n",
    "## Objective: Extract real-game statistics to parameterize an ABM for CS:GO economy decisions\n",
    "\n",
    "This notebook analyzes real CS:GO match data to determine key distributions and probabilities for:\n",
    "1. **Win probability** based on team equipment value/spending and other factors \n",
    "Using different methods\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ed41527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Querying round outcome data with HLTV rankings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_1968\\4077545205.py:63: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  classification_data = pd.read_sql(classification_query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieved 1,822,909 rounds from 68,776 matches\n",
      "\n",
      "üìä Data Summary:\n",
      "  Win rate (Team 1): 0.512\n",
      "  Avg equipment diff: $59\n",
      "  Avg rank diff: 10.1\n",
      "  Missing values: 263010\n",
      "\n",
      "üîç Sample data:\n",
      "   round_id  id_demo_exports  round_num  team1_winner  t1_eq_val  \\\n",
      "0     13465                1          1         False      860.0   \n",
      "1     13468                1          2          True     3250.0   \n",
      "2     13471                1          3          True     4450.0   \n",
      "3     13474                1          4          True     5060.0   \n",
      "4     13477                1          5          True     5610.0   \n",
      "5     13480                1          6          True     5550.0   \n",
      "6     13483                1          7         False     5730.0   \n",
      "7     13486                1          8         False     5530.0   \n",
      "8     13489                1          9         False     4930.0   \n",
      "9     13492                1         10          True     4890.0   \n",
      "\n",
      "   t1_money_spent  t1_rank  t2_eq_val  t2_money_spent  t2_rank  diff_eq_val  \\\n",
      "0          3300.0      169      800.0          3400.0      125         60.0   \n",
      "1         15450.0      169     3650.0         19650.0      125       -400.0   \n",
      "2         17050.0      169     2440.0          7700.0      125       2010.0   \n",
      "3          4300.0      169      200.0           400.0      125       4860.0   \n",
      "4          9050.0      169     4540.0         21700.0      125       1070.0   \n",
      "5         12450.0      169     1300.0          5700.0      125       4250.0   \n",
      "6          3950.0      169     5290.0         25250.0      125        440.0   \n",
      "7         26850.0      169     5670.0         13150.0      125       -140.0   \n",
      "8         19500.0      169     5640.0         11100.0      125       -710.0   \n",
      "9         23750.0      169     6030.0         11300.0      125      -1140.0   \n",
      "\n",
      "   diff_money_spent  diff_rank  \n",
      "0            -100.0        -44  \n",
      "1           -4200.0        -44  \n",
      "2            9350.0        -44  \n",
      "3            3900.0        -44  \n",
      "4          -12650.0        -44  \n",
      "5            6750.0        -44  \n",
      "6          -21300.0        -44  \n",
      "7           13700.0        -44  \n",
      "8            8400.0        -44  \n",
      "9           12450.0        -44  \n"
     ]
    }
   ],
   "source": [
    "# 1. Query Classification Data with Skill Controls\n",
    "\n",
    "print(\"üîç Querying round outcome data with HLTV rankings...\")\n",
    "\n",
    "classification_query = \"\"\"\n",
    "WITH team_round_data AS (\n",
    "    SELECT \n",
    "        r.id as round_id,\n",
    "        r.id_demo_exports as id_demo_exports,\n",
    "        r.round_num,\n",
    "        r.team1_winner,\n",
    "        -- Team 1 aggregated metrics\n",
    "        AVG(CASE WHEN pr.team = 1 THEN pr.eq_val_fte END) as t1_eq_val,\n",
    "        SUM(CASE WHEN pr.team = 1 THEN pe.money_spent END) as t1_money_spent,\n",
    "        -- Team 2 aggregated metrics  \n",
    "        AVG(CASE WHEN pr.team = 2 THEN pr.eq_val_fte END) as t2_eq_val,\n",
    "        SUM(CASE WHEN pr.team = 2 THEN pe.money_spent END) as t2_money_spent,\n",
    "        -- HLTV rankings\n",
    "        hmi.team_1_id,\n",
    "        hmi.team_2_id,\n",
    "        hmi.event_id\n",
    "    FROM rounds_ed r\n",
    "    JOIN player_round_ed pr ON r.id = pr.round_id\n",
    "    LEFT JOIN player_economy_ed pe ON pr.id = pe.player_round_id\n",
    "    LEFT JOIN hltv_match_info hmi ON r.match_id = hmi.match_id\n",
    "    WHERE r.team1_winner IS NOT NULL\n",
    "        AND pr.team IN (1, 2)\n",
    "        AND pr.eq_val_fte IS NOT NULL\n",
    "        AND hmi.event_id IS NOT NULL\n",
    "    GROUP BY r.id, r.id_demo_exports, r.round_num, r.team1_winner,\n",
    "             hmi.team_1_id, hmi.team_2_id, hmi.event_id\n",
    "    HAVING COUNT(CASE WHEN pr.team = 1 THEN 1 END) = 5 \n",
    "       AND COUNT(CASE WHEN pr.team = 2 THEN 1 END) = 5\n",
    ")\n",
    "SELECT \n",
    "    trd.round_id,\n",
    "    trd.id_demo_exports,\n",
    "    trd.round_num,\n",
    "    trd.team1_winner,\n",
    "    -- Team 1 metrics\n",
    "    trd.t1_eq_val,\n",
    "    trd.t1_money_spent,\n",
    "    CAST(het1.rank_during AS INTEGER) as t1_rank,\n",
    "    -- Team 2 metrics\n",
    "    trd.t2_eq_val,\n",
    "    trd.t2_money_spent,\n",
    "    CAST(het2.rank_during AS INTEGER) as t2_rank,\n",
    "    -- Differences (advantages for team 1)\n",
    "    (trd.t1_eq_val - trd.t2_eq_val) as diff_eq_val,\n",
    "    (trd.t1_money_spent - trd.t2_money_spent) as diff_money_spent,\n",
    "    (CAST(het2.rank_during AS INTEGER) - CAST(het1.rank_during AS INTEGER)) as diff_rank\n",
    "FROM team_round_data trd\n",
    "LEFT JOIN hltv_events_teams het1 ON het1.team_id = trd.team_1_id AND het1.event_id = trd.event_id\n",
    "LEFT JOIN hltv_events_teams het2 ON het2.team_id = trd.team_2_id AND het2.event_id = trd.event_id\n",
    "WHERE het1.rank_during IS NOT NULL \n",
    "    AND het2.rank_during IS NOT NULL\n",
    "    AND trd.t1_eq_val IS NOT NULL \n",
    "    AND trd.t2_eq_val IS NOT NULL\n",
    "ORDER BY trd.id_demo_exports, trd.round_num\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    classification_data = pd.read_sql(classification_query, conn)\n",
    "    print(f\"‚úÖ Retrieved {len(classification_data):,} rounds from {classification_data['id_demo_exports'].nunique():,} matches\")\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\nüìä Data Summary:\")\n",
    "    print(f\"  Win rate (Team 1): {classification_data['team1_winner'].mean():.3f}\")\n",
    "    print(f\"  Avg equipment diff: ${classification_data['diff_eq_val'].mean():.0f}\")\n",
    "    print(f\"  Avg rank diff: {classification_data['diff_rank'].mean():.1f}\")\n",
    "    print(f\"  Missing values: {classification_data.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Preview data\n",
    "    print(\"\\nüîç Sample data:\")\n",
    "    print(classification_data.head(10))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Query error: {e}\")\n",
    "    classification_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5877102d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Preparing features for classification models...\n",
      "‚úÖ Feature matrix created: 1,822,909 samples √ó 12 features\n",
      "\n",
      "üìã Features included:\n",
      "  1. t1_eq_val\n",
      "  2. t1_money_spent\n",
      "  3. t1_rank\n",
      "  4. t2_eq_val\n",
      "  5. t2_money_spent\n",
      "  6. t2_rank\n",
      "  7. diff_eq_val\n",
      "  8. diff_money_spent\n",
      "  9. diff_rank\n",
      "  10. eq_x_rank\n",
      "  11. spending_x_rank\n",
      "  12. eq_ratio\n",
      "\n",
      "‚öñÔ∏è Class Distribution:\n",
      "  Team 1 Wins (1): 933,772 (51.2%)\n",
      "  Team 1 Loses (0): 889,137 (48.8%)\n",
      "\n",
      "üìä Feature Statistics:\n",
      "‚úÖ Feature matrix created: 1,822,909 samples √ó 12 features\n",
      "\n",
      "üìã Features included:\n",
      "  1. t1_eq_val\n",
      "  2. t1_money_spent\n",
      "  3. t1_rank\n",
      "  4. t2_eq_val\n",
      "  5. t2_money_spent\n",
      "  6. t2_rank\n",
      "  7. diff_eq_val\n",
      "  8. diff_money_spent\n",
      "  9. diff_rank\n",
      "  10. eq_x_rank\n",
      "  11. spending_x_rank\n",
      "  12. eq_ratio\n",
      "\n",
      "‚öñÔ∏è Class Distribution:\n",
      "  Team 1 Wins (1): 933,772 (51.2%)\n",
      "  Team 1 Loses (0): 889,137 (48.8%)\n",
      "\n",
      "üìä Feature Statistics:\n",
      "        t1_eq_val  t1_money_spent     t1_rank   t2_eq_val  t2_money_spent  \\\n",
      "count  1822909.00      1822909.00  1822909.00  1822909.00      1822909.00   \n",
      "mean      4143.88        13616.58       70.42     4084.84        13651.78   \n",
      "std       1953.97         7806.77       57.61     1966.48         7861.87   \n",
      "min        160.00       -37350.00        1.00      160.00       -58850.00   \n",
      "25%       2680.00         7250.00       28.00     2380.00         7200.00   \n",
      "50%       4890.00        13700.00       54.00     4840.00        13800.00   \n",
      "75%       5550.00        19500.00       97.00     5530.00        19650.00   \n",
      "max       8110.00        79350.00      395.00     8070.00        74200.00   \n",
      "\n",
      "          t2_rank  diff_eq_val  diff_money_spent   diff_rank   eq_x_rank  \\\n",
      "count  1822909.00   1822909.00        1822909.00  1822909.00  1822909.00   \n",
      "mean        80.51        59.04            -30.40       10.09    10295.84   \n",
      "std         60.91      2608.97          10237.95       56.01   150108.11   \n",
      "min          1.00     -7200.00         -40650.00     -335.00 -1778850.00   \n",
      "25%         34.00     -1400.00          -7950.00      -16.00   -24700.00   \n",
      "50%         65.00        10.00              0.00        8.00      540.00   \n",
      "75%        113.00      1530.00           7900.00       35.00    37440.00   \n",
      "max        390.00      7120.00          55000.00      324.00  1938440.00   \n",
      "\n",
      "       spending_x_rank    eq_ratio  \n",
      "count       1822909.00  1822909.00  \n",
      "mean          -3845.93        2.19  \n",
      "std          584189.93        4.13  \n",
      "min        -8432000.00        0.03  \n",
      "25%         -147200.00        0.76  \n",
      "50%               0.00        1.01  \n",
      "75%          149400.00        1.37  \n",
      "max         8288000.00       41.74  \n",
      "        t1_eq_val  t1_money_spent     t1_rank   t2_eq_val  t2_money_spent  \\\n",
      "count  1822909.00      1822909.00  1822909.00  1822909.00      1822909.00   \n",
      "mean      4143.88        13616.58       70.42     4084.84        13651.78   \n",
      "std       1953.97         7806.77       57.61     1966.48         7861.87   \n",
      "min        160.00       -37350.00        1.00      160.00       -58850.00   \n",
      "25%       2680.00         7250.00       28.00     2380.00         7200.00   \n",
      "50%       4890.00        13700.00       54.00     4840.00        13800.00   \n",
      "75%       5550.00        19500.00       97.00     5530.00        19650.00   \n",
      "max       8110.00        79350.00      395.00     8070.00        74200.00   \n",
      "\n",
      "          t2_rank  diff_eq_val  diff_money_spent   diff_rank   eq_x_rank  \\\n",
      "count  1822909.00   1822909.00        1822909.00  1822909.00  1822909.00   \n",
      "mean        80.51        59.04            -30.40       10.09    10295.84   \n",
      "std         60.91      2608.97          10237.95       56.01   150108.11   \n",
      "min          1.00     -7200.00         -40650.00     -335.00 -1778850.00   \n",
      "25%         34.00     -1400.00          -7950.00      -16.00   -24700.00   \n",
      "50%         65.00        10.00              0.00        8.00      540.00   \n",
      "75%        113.00      1530.00           7900.00       35.00    37440.00   \n",
      "max        390.00      7120.00          55000.00      324.00  1938440.00   \n",
      "\n",
      "       spending_x_rank    eq_ratio  \n",
      "count       1822909.00  1822909.00  \n",
      "mean          -3845.93        2.19  \n",
      "std          584189.93        4.13  \n",
      "min        -8432000.00        0.03  \n",
      "25%         -147200.00        0.76  \n",
      "50%               0.00        1.01  \n",
      "75%          149400.00        1.37  \n",
      "max         8288000.00       41.74  \n"
     ]
    }
   ],
   "source": [
    "# 2. Prepare Features for Classification\n",
    "\n",
    "print(\"üîß Preparing features for classification models...\")\n",
    "\n",
    "if classification_data is not None and len(classification_data) > 0:\n",
    "    # Create feature matrix\n",
    "    feature_columns = [\n",
    "        't1_eq_val', 't1_money_spent', 't1_rank',\n",
    "        't2_eq_val', 't2_money_spent', 't2_rank',\n",
    "        'diff_eq_val', 'diff_money_spent', 'diff_rank'\n",
    "    ]\n",
    "    \n",
    "    X = classification_data[feature_columns].copy()\n",
    "    y = classification_data['team1_winner'].astype(int)\n",
    "    \n",
    "    # Handle any missing values\n",
    "    X = X.fillna(X.median())\n",
    "    \n",
    "    # Add interaction terms (critical for understanding combined effects)\n",
    "    X['eq_x_rank'] = X['diff_eq_val'] * X['diff_rank']\n",
    "    X['spending_x_rank'] = X['diff_money_spent'] * X['diff_rank']\n",
    "    X['eq_ratio'] = X['t1_eq_val'] / (X['t2_eq_val'] + 1)\n",
    "    \n",
    "    print(f\"‚úÖ Feature matrix created: {X.shape[0]:,} samples √ó {X.shape[1]} features\")\n",
    "    print(f\"\\nüìã Features included:\")\n",
    "    for i, col in enumerate(X.columns, 1):\n",
    "        print(f\"  {i}. {col}\")\n",
    "    \n",
    "    # Check for class imbalance\n",
    "    class_distribution = y.value_counts()\n",
    "    print(f\"\\n‚öñÔ∏è Class Distribution:\")\n",
    "    print(f\"  Team 1 Wins (1): {class_distribution.get(1, 0):,} ({class_distribution.get(1, 0)/len(y)*100:.1f}%)\")\n",
    "    print(f\"  Team 1 Loses (0): {class_distribution.get(0, 0):,} ({class_distribution.get(0, 0)/len(y)*100:.1f}%)\")\n",
    "    \n",
    "    # Descriptive statistics for features\n",
    "    print(f\"\\nüìä Feature Statistics:\")\n",
    "    print(X.describe().round(2))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for feature preparation\")\n",
    "    X = None\n",
    "    y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc194f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Building classification models...\n",
      "üìä Data split:\n",
      "  Training set: 1,458,327 samples\n",
      "  Test set: 364,582 samples\n",
      "üìä Data split:\n",
      "  Training set: 1,458,327 samples\n",
      "  Test set: 364,582 samples\n",
      "\n",
      "======================================================================\n",
      "üèÜ MODEL COMPARISON - 5-Fold Stratified Cross-Validation\n",
      "======================================================================\n",
      "\n",
      "üìà Training Logistic Regression...\n",
      "\n",
      "======================================================================\n",
      "üèÜ MODEL COMPARISON - 5-Fold Stratified Cross-Validation\n",
      "======================================================================\n",
      "\n",
      "üìà Training Logistic Regression...\n",
      "  Cross-Validation Accuracy: 0.6556 (¬±0.0013)\n",
      "  Cross-Validation ROC-AUC:  0.7384 (¬±0.0011)\n",
      "  Cross-Validation F1-Score: 0.6459 (¬±0.0013)\n",
      "  Test Set Accuracy:         0.6552\n",
      "  Test Set ROC-AUC:          0.7374\n",
      "  Test Set Precision:        0.6818\n",
      "  Test Set Recall:           0.6130\n",
      "  Test Set F1-Score:         0.6456\n",
      "\n",
      "üìà Training Random Forest...\n",
      "  Cross-Validation Accuracy: 0.6556 (¬±0.0013)\n",
      "  Cross-Validation ROC-AUC:  0.7384 (¬±0.0011)\n",
      "  Cross-Validation F1-Score: 0.6459 (¬±0.0013)\n",
      "  Test Set Accuracy:         0.6552\n",
      "  Test Set ROC-AUC:          0.7374\n",
      "  Test Set Precision:        0.6818\n",
      "  Test Set Recall:           0.6130\n",
      "  Test Set F1-Score:         0.6456\n",
      "\n",
      "üìà Training Random Forest...\n",
      "  Cross-Validation Accuracy: 0.6600 (¬±0.0013)\n",
      "  Cross-Validation ROC-AUC:  0.7430 (¬±0.0011)\n",
      "  Cross-Validation F1-Score: 0.6610 (¬±0.0023)\n",
      "  Test Set Accuracy:         0.6599\n",
      "  Test Set ROC-AUC:          0.7425\n",
      "  Test Set Precision:        0.6753\n",
      "  Test Set Recall:           0.6474\n",
      "  Test Set F1-Score:         0.6610\n",
      "\n",
      "üìà Training Gradient Boosting...\n",
      "  Cross-Validation Accuracy: 0.6600 (¬±0.0013)\n",
      "  Cross-Validation ROC-AUC:  0.7430 (¬±0.0011)\n",
      "  Cross-Validation F1-Score: 0.6610 (¬±0.0023)\n",
      "  Test Set Accuracy:         0.6599\n",
      "  Test Set ROC-AUC:          0.7425\n",
      "  Test Set Precision:        0.6753\n",
      "  Test Set Recall:           0.6474\n",
      "  Test Set F1-Score:         0.6610\n",
      "\n",
      "üìà Training Gradient Boosting...\n"
     ]
    }
   ],
   "source": [
    "# 3. Build and Compare Multiple Classification Models\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"ü§ñ Building classification models...\")\n",
    "\n",
    "if X is not None and y is not None:\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä Data split:\")\n",
    "    print(f\"  Training set: {len(X_train):,} samples\")\n",
    "    print(f\"  Test set: {len(X_test):,} samples\")\n",
    "    \n",
    "    # Standardize features (important for logistic regression)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Define models to compare\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(\n",
    "            max_iter=1000, \n",
    "            class_weight='balanced',\n",
    "            random_state=42\n",
    "        ),\n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            min_samples_split=50,\n",
    "            min_samples_leaf=20,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            min_samples_split=50,\n",
    "            min_samples_leaf=20,\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üèÜ MODEL COMPARISON - 5-Fold Stratified Cross-Validation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    model_results = {}\n",
    "    trained_models = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nüìà Training {name}...\")\n",
    "        \n",
    "        # Use scaled data for Logistic Regression, original for tree-based\n",
    "        if name == 'Logistic Regression':\n",
    "            X_train_use = X_train_scaled\n",
    "            X_test_use = X_test_scaled\n",
    "        else:\n",
    "            X_train_use = X_train\n",
    "            X_test_use = X_test\n",
    "        \n",
    "        # Cross-validation scores\n",
    "        cv_accuracy = cross_val_score(model, X_train_use, y_train, cv=cv, scoring='accuracy')\n",
    "        cv_roc_auc = cross_val_score(model, X_train_use, y_train, cv=cv, scoring='roc_auc')\n",
    "        cv_f1 = cross_val_score(model, X_train_use, y_train, cv=cv, scoring='f1')\n",
    "        \n",
    "        # Train on full training set\n",
    "        model.fit(X_train_use, y_train)\n",
    "        \n",
    "        # Test set predictions\n",
    "        y_pred = model.predict(X_test_use)\n",
    "        y_prob = model.predict_proba(X_test_use)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        test_accuracy = accuracy_score(y_test, y_pred)\n",
    "        test_roc_auc = roc_auc_score(y_test, y_prob)\n",
    "        test_precision = precision_score(y_test, y_pred)\n",
    "        test_recall = recall_score(y_test, y_pred)\n",
    "        test_f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        model_results[name] = {\n",
    "            'cv_accuracy_mean': cv_accuracy.mean(),\n",
    "            'cv_accuracy_std': cv_accuracy.std(),\n",
    "            'cv_roc_auc_mean': cv_roc_auc.mean(),\n",
    "            'cv_roc_auc_std': cv_roc_auc.std(),\n",
    "            'cv_f1_mean': cv_f1.mean(),\n",
    "            'cv_f1_std': cv_f1.std(),\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'test_roc_auc': test_roc_auc,\n",
    "            'test_precision': test_precision,\n",
    "            'test_recall': test_recall,\n",
    "            'test_f1': test_f1,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_prob\n",
    "        }\n",
    "        \n",
    "        trained_models[name] = model\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"  Cross-Validation Accuracy: {cv_accuracy.mean():.4f} (¬±{cv_accuracy.std()*2:.4f})\")\n",
    "        print(f\"  Cross-Validation ROC-AUC:  {cv_roc_auc.mean():.4f} (¬±{cv_roc_auc.std()*2:.4f})\")\n",
    "        print(f\"  Cross-Validation F1-Score: {cv_f1.mean():.4f} (¬±{cv_f1.std()*2:.4f})\")\n",
    "        print(f\"  Test Set Accuracy:         {test_accuracy:.4f}\")\n",
    "        print(f\"  Test Set ROC-AUC:          {test_roc_auc:.4f}\")\n",
    "        print(f\"  Test Set Precision:        {test_precision:.4f}\")\n",
    "        print(f\"  Test Set Recall:           {test_recall:.4f}\")\n",
    "        print(f\"  Test Set F1-Score:         {test_f1:.4f}\")\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    comparison_df = pd.DataFrame(model_results).T\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä MODEL COMPARISON SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(comparison_df[['cv_roc_auc_mean', 'test_roc_auc', 'test_accuracy', 'test_f1']].round(4))\n",
    "    \n",
    "    # Identify best model\n",
    "    best_model_name = comparison_df['test_roc_auc'].idxmax()\n",
    "    print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "    print(f\"   Test ROC-AUC: {comparison_df.loc[best_model_name, 'test_roc_auc']:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for model training\")\n",
    "    trained_models = {}\n",
    "    model_results = {}\n",
    "    best_model_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9feac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Detailed evaluation of best model: None\n",
      "‚ùå No model results available for evaluation\n"
     ]
    }
   ],
   "source": [
    "# 4. Detailed Evaluation of Best Model\n",
    "\n",
    "print(f\"üîç Detailed evaluation of best model: {best_model_name}\")\n",
    "\n",
    "if best_model_name is not None and len(model_results) > 0:\n",
    "    # Get best model predictions\n",
    "    best_predictions = model_results[best_model_name]['predictions']\n",
    "    best_probabilities = model_results[best_model_name]['probabilities']\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(\"\\nüìä CONFUSION MATRIX\")\n",
    "    print(\"=\"*50)\n",
    "    cm = confusion_matrix(y_test, best_predictions)\n",
    "    print(f\"\\n                 Predicted\")\n",
    "    print(f\"              Loss (0)  Win (1)\")\n",
    "    print(f\"Actual Loss    {cm[0,0]:6d}   {cm[0,1]:6d}\")\n",
    "    print(f\"Actual Win     {cm[1,0]:6d}   {cm[1,1]:6d}\")\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    \n",
    "    print(f\"\\nüìà Additional Metrics:\")\n",
    "    print(f\"  True Negatives:  {tn:,}\")\n",
    "    print(f\"  False Positives: {fp:,}\")\n",
    "    print(f\"  False Negatives: {fn:,}\")\n",
    "    print(f\"  True Positives:  {tp:,}\")\n",
    "    print(f\"  Specificity:     {specificity:.4f}\")\n",
    "    print(f\"  Sensitivity:     {sensitivity:.4f}\")\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"\\nüìã CLASSIFICATION REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(y_test, best_predictions, \n",
    "                                target_names=['Team 1 Loss', 'Team 1 Win'],\n",
    "                                digits=4))\n",
    "    \n",
    "    # Plot ROC Curve\n",
    "    print(\"\\nüìâ Plotting ROC Curve...\")\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, best_probabilities)\n",
    "    roc_auc = roc_auc_score(y_test, best_probabilities)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'{best_model_name} (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title(f'ROC Curve - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\", fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot Probability Distribution\n",
    "    print(\"\\nüìä Plotting Prediction Probability Distribution...\")\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Histogram of probabilities by actual class\n",
    "    win_probs = best_probabilities[y_test == 1]\n",
    "    loss_probs = best_probabilities[y_test == 0]\n",
    "    \n",
    "    ax1.hist(win_probs, bins=50, alpha=0.7, label='Actual Wins', color='green', edgecolor='black')\n",
    "    ax1.hist(loss_probs, bins=50, alpha=0.7, label='Actual Losses', color='red', edgecolor='black')\n",
    "    ax1.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "    ax1.set_xlabel('Predicted Probability of Win', fontsize=12)\n",
    "    ax1.set_ylabel('Frequency', fontsize=12)\n",
    "    ax1.set_title('Distribution of Predicted Probabilities', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot comparison\n",
    "    ax2.boxplot([loss_probs, win_probs], labels=['Actual Losses', 'Actual Wins'])\n",
    "    ax2.axhline(y=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "    ax2.set_ylabel('Predicted Probability of Win', fontsize=12)\n",
    "    ax2.set_title('Probability Distribution by Actual Outcome', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.legend(fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No model results available for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46ab635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Analyzing Feature Importance...\n",
      "‚ùå No model available for feature importance analysis\n"
     ]
    }
   ],
   "source": [
    "# 5. Feature Importance Analysis\n",
    "\n",
    "print(\"üéØ Analyzing Feature Importance...\")\n",
    "\n",
    "if best_model_name is not None and len(trained_models) > 0:\n",
    "    best_model = trained_models[best_model_name]\n",
    "    \n",
    "    if best_model_name == 'Logistic Regression':\n",
    "        # For logistic regression, analyze coefficients\n",
    "        print(\"\\nüìä LOGISTIC REGRESSION COEFFICIENTS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        coefficients = best_model.coef_[0]\n",
    "        feature_names = X.columns\n",
    "        \n",
    "        # Create importance dataframe\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'coefficient': coefficients,\n",
    "            'abs_coefficient': np.abs(coefficients),\n",
    "            'odds_ratio': np.exp(coefficients)\n",
    "        }).sort_values('abs_coefficient', ascending=False)\n",
    "        \n",
    "        print(\"\\nFeature Coefficients (sorted by absolute value):\")\n",
    "        print(importance_df.to_string(index=False))\n",
    "        \n",
    "        print(\"\\nüí° Interpretation:\")\n",
    "        print(\"  - Positive coefficient: increases win probability\")\n",
    "        print(\"  - Negative coefficient: decreases win probability\")\n",
    "        print(\"  - Odds ratio > 1: multiplicative increase in odds\")\n",
    "        print(\"  - Odds ratio < 1: multiplicative decrease in odds\")\n",
    "        \n",
    "        # Plot coefficients\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        colors = ['green' if x > 0 else 'red' for x in importance_df['coefficient']]\n",
    "        plt.barh(importance_df['feature'], importance_df['coefficient'], color=colors, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Coefficient Value', fontsize=12)\n",
    "        plt.ylabel('Feature', fontsize=12)\n",
    "        plt.title('Logistic Regression Coefficients', fontsize=14, fontweight='bold')\n",
    "        plt.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    elif best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
    "        # For tree-based models, use feature importance\n",
    "        print(f\"\\nüìä {best_model_name.upper()} FEATURE IMPORTANCE\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        importances = best_model.feature_importances_\n",
    "        feature_names = X.columns\n",
    "        \n",
    "        # Create importance dataframe\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances,\n",
    "            'importance_pct': importances * 100\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nFeature Importance (sorted by importance):\")\n",
    "        print(importance_df.to_string(index=False))\n",
    "        \n",
    "        print(\"\\nüí° Interpretation:\")\n",
    "        print(\"  - Higher importance: feature contributes more to predictions\")\n",
    "        print(\"  - Importance measures average reduction in impurity\")\n",
    "        \n",
    "        # Plot importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.barh(importance_df['feature'], importance_df['importance'], \n",
    "                color='steelblue', alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Feature Importance', fontsize=12)\n",
    "        plt.ylabel('Feature', fontsize=12)\n",
    "        plt.title(f'{best_model_name} Feature Importance', fontsize=14, fontweight='bold')\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Summary of key insights\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîë KEY INSIGHTS FROM FEATURE IMPORTANCE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    top_features = importance_df.head(5)\n",
    "    print(\"\\nTop 5 Most Important Features:\")\n",
    "    for idx, row in top_features.iterrows():\n",
    "        print(f\"  {row['feature']}\")\n",
    "    \n",
    "    print(\"\\nüí° For ABM Implementation:\")\n",
    "    print(\"  - Focus calibration on top 3-5 features\")\n",
    "    print(\"  - Equipment differences and rank interactions are likely critical\")\n",
    "    print(\"  - Use these features for scenario sensitivity analysis\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No model available for feature importance analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81556e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analyzing Win Probability across Feature Ranges...\n",
      "‚ùå No data available for calibration analysis\n"
     ]
    }
   ],
   "source": [
    "# 6. Model Calibration Analysis - Win Probability by Feature Values\n",
    "\n",
    "print(\"üìä Analyzing Win Probability across Feature Ranges...\")\n",
    "\n",
    "if classification_data is not None and best_model_name is not None:\n",
    "    \n",
    "    # Prepare data for predictions\n",
    "    if best_model_name == 'Logistic Regression':\n",
    "        X_for_pred = scaler.transform(X)\n",
    "    else:\n",
    "        X_for_pred = X\n",
    "    \n",
    "    # Get all predictions\n",
    "    all_predictions = trained_models[best_model_name].predict_proba(X_for_pred)[:, 1]\n",
    "    \n",
    "    # Add predictions to original data\n",
    "    analysis_df = classification_data.copy()\n",
    "    analysis_df['predicted_win_prob'] = all_predictions\n",
    "    \n",
    "    print(\"\\nüìà WIN PROBABILITY BY EQUIPMENT ADVANTAGE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Bin equipment advantage\n",
    "    eq_bins = [-float('inf'), -3000, -1500, -500, 500, 1500, 3000, float('inf')]\n",
    "    eq_labels = ['< -$3000', '-$3000 to -$1500', '-$1500 to -$500', \n",
    "                 '-$500 to $500', '$500 to $1500', '$1500 to $3000', '> $3000']\n",
    "    \n",
    "    analysis_df['eq_advantage_bin'] = pd.cut(analysis_df['diff_eq_val'], \n",
    "                                            bins=eq_bins, labels=eq_labels)\n",
    "    \n",
    "    eq_analysis = analysis_df.groupby('eq_advantage_bin').agg({\n",
    "        'team1_winner': ['count', 'mean'],\n",
    "        'predicted_win_prob': 'mean',\n",
    "        'diff_eq_val': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    eq_analysis.columns = ['Count', 'Actual_Win_Rate', 'Predicted_Win_Prob', 'Avg_Eq_Diff']\n",
    "    print(eq_analysis)\n",
    "    \n",
    "    # Plot equipment advantage vs win probability\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    x_pos = range(len(eq_analysis))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar([p - width/2 for p in x_pos], eq_analysis['Actual_Win_Rate'], \n",
    "            width, label='Actual Win Rate', alpha=0.8, color='steelblue', edgecolor='black')\n",
    "    plt.bar([p + width/2 for p in x_pos], eq_analysis['Predicted_Win_Prob'], \n",
    "            width, label='Predicted Win Prob', alpha=0.8, color='orange', edgecolor='black')\n",
    "    \n",
    "    plt.xlabel('Equipment Advantage Range', fontsize=12)\n",
    "    plt.ylabel('Win Probability', fontsize=12)\n",
    "    plt.title('Win Probability by Equipment Advantage (Team 1)', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(x_pos, eq_analysis.index, rotation=45, ha='right')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìà WIN PROBABILITY BY RANK ADVANTAGE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Bin rank advantage\n",
    "    rank_bins = [-float('inf'), -10, -5, -2, 2, 5, 10, float('inf')]\n",
    "    rank_labels = ['Much Weaker', 'Weaker', 'Slightly Weaker', \n",
    "                   'Even', 'Slightly Stronger', 'Stronger', 'Much Stronger']\n",
    "    \n",
    "    analysis_df['rank_advantage_bin'] = pd.cut(analysis_df['diff_rank'], \n",
    "                                              bins=rank_bins, labels=rank_labels)\n",
    "    \n",
    "    rank_analysis = analysis_df.groupby('rank_advantage_bin').agg({\n",
    "        'team1_winner': ['count', 'mean'],\n",
    "        'predicted_win_prob': 'mean',\n",
    "        'diff_rank': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    rank_analysis.columns = ['Count', 'Actual_Win_Rate', 'Predicted_Win_Prob', 'Avg_Rank_Diff']\n",
    "    print(rank_analysis)\n",
    "    \n",
    "    # Plot rank advantage vs win probability\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    x_pos = range(len(rank_analysis))\n",
    "    \n",
    "    plt.bar([p - width/2 for p in x_pos], rank_analysis['Actual_Win_Rate'], \n",
    "            width, label='Actual Win Rate', alpha=0.8, color='green', edgecolor='black')\n",
    "    plt.bar([p + width/2 for p in x_pos], rank_analysis['Predicted_Win_Prob'], \n",
    "            width, label='Predicted Win Prob', alpha=0.8, color='red', edgecolor='black')\n",
    "    \n",
    "    plt.xlabel('Rank Advantage Category', fontsize=12)\n",
    "    plt.ylabel('Win Probability', fontsize=12)\n",
    "    plt.title('Win Probability by Rank Advantage (Team 1)', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(x_pos, rank_analysis.index, rotation=45, ha='right')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Interaction analysis\n",
    "    print(\"\\nüìä WIN PROBABILITY MATRIX: Equipment √ó Rank\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create simplified bins for matrix\n",
    "    analysis_df['eq_simple'] = pd.cut(analysis_df['diff_eq_val'], \n",
    "                                     bins=[-float('inf'), -1500, 0, 1500, float('inf')],\n",
    "                                     labels=['Large Disadv', 'Small Disadv', 'Small Adv', 'Large Adv'])\n",
    "    \n",
    "    analysis_df['rank_simple'] = pd.cut(analysis_df['diff_rank'], \n",
    "                                       bins=[-float('inf'), -5, 0, 5, float('inf')],\n",
    "                                       labels=['Weaker', 'Even', 'Stronger', 'Much Stronger'])\n",
    "    \n",
    "    interaction_matrix = pd.crosstab(\n",
    "        analysis_df['rank_simple'],\n",
    "        analysis_df['eq_simple'],\n",
    "        values=analysis_df['predicted_win_prob'],\n",
    "        aggfunc='mean'\n",
    "    ).round(3)\n",
    "    \n",
    "    print(\"\\nPredicted Win Probability Matrix:\")\n",
    "    print(\"(Rows: Rank Advantage, Columns: Equipment Advantage)\")\n",
    "    print(interaction_matrix)\n",
    "    \n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(interaction_matrix, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "                center=0.5, vmin=0, vmax=1, cbar_kws={'label': 'Win Probability'},\n",
    "                linewidths=1, linecolor='black')\n",
    "    plt.title('Win Probability Heatmap: Rank √ó Equipment', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Rank Advantage', fontsize=12)\n",
    "    plt.xlabel('Equipment Advantage', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data available for calibration analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b06554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Exporting model for ABM integration...\n",
      "‚ùå No model available for export\n"
     ]
    }
   ],
   "source": [
    "# 7. Export Model and ABM Integration Code\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üíæ Exporting model for ABM integration...\")\n",
    "\n",
    "if best_model_name is not None and len(trained_models) > 0:\n",
    "    \n",
    "    # Prepare export package\n",
    "    export_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    abm_model_package = {\n",
    "        'model': trained_models[best_model_name],\n",
    "        'scaler': scaler if best_model_name == 'Logistic Regression' else None,\n",
    "        'feature_names': list(X.columns),\n",
    "        'model_type': best_model_name,\n",
    "        'performance': {\n",
    "            'test_accuracy': model_results[best_model_name]['test_accuracy'],\n",
    "            'test_roc_auc': model_results[best_model_name]['test_roc_auc'],\n",
    "            'test_f1': model_results[best_model_name]['test_f1'],\n",
    "            'cv_roc_auc_mean': model_results[best_model_name]['cv_roc_auc_mean'],\n",
    "            'cv_roc_auc_std': model_results[best_model_name]['cv_roc_auc_std']\n",
    "        },\n",
    "        'metadata': {\n",
    "            'export_timestamp': export_timestamp,\n",
    "            'training_samples': len(X_train),\n",
    "            'test_samples': len(X_test),\n",
    "            'feature_count': X.shape[1],\n",
    "            'database': 'csgo_parsed',\n",
    "            'description': 'CS:GO win probability classifier with HLTV ranking controls'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Export as pickle\n",
    "    pickle_filename = f\"csgo_win_classifier_{export_timestamp}.pkl\"\n",
    "    with open(pickle_filename, 'wb') as f:\n",
    "        pickle.dump(abm_model_package, f)\n",
    "    \n",
    "    print(f\"‚úÖ Model exported to: {pickle_filename}\")\n",
    "    \n",
    "    # Export metadata as JSON\n",
    "    json_metadata = {\n",
    "        'model_type': best_model_name,\n",
    "        'performance': abm_model_package['performance'],\n",
    "        'metadata': abm_model_package['metadata'],\n",
    "        'feature_names': abm_model_package['feature_names']\n",
    "    }\n",
    "    \n",
    "    json_filename = f\"csgo_win_classifier_metadata_{export_timestamp}.json\"\n",
    "    with open(json_filename, 'w') as f:\n",
    "        json.dump(json_metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Metadata exported to: {json_filename}\")\n",
    "    \n",
    "    # Create ABM integration example code\n",
    "    integration_code = f'''\n",
    "# ============================================================================\n",
    "# CS:GO ABM Win Probability Calculator - Integration Code\n",
    "# Model: {best_model_name}\n",
    "# Exported: {export_timestamp}\n",
    "# Performance: ROC-AUC = {model_results[best_model_name]['test_roc_auc']:.4f}\n",
    "# ============================================================================\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "class CSGOWinProbabilityCalculator:\n",
    "    \"\"\"\n",
    "    Win probability calculator for CS:GO Agent-Based Model\n",
    "    Uses {best_model_name} trained on real match data with HLTV rankings\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path='{pickle_filename}'):\n",
    "        \"\"\"Load the trained model\"\"\"\n",
    "        with open(model_path, 'rb') as f:\n",
    "            package = pickle.load(f)\n",
    "            self.model = package['model']\n",
    "            self.scaler = package['scaler']\n",
    "            self.feature_names = package['feature_names']\n",
    "            self.model_type = package['model_type']\n",
    "            self.performance = package['performance']\n",
    "        \n",
    "        print(f\"üìä Loaded {{self.model_type}}\")\n",
    "        print(f\"   ROC-AUC: {{self.performance['test_roc_auc']:.4f}}\")\n",
    "    \n",
    "    def predict_win_probability(self, t1_eq_val, t1_money_spent, t1_rank,\n",
    "                                t2_eq_val, t2_money_spent, t2_rank):\n",
    "        \"\"\"\n",
    "        Calculate Team 1 win probability\n",
    "        \n",
    "        Args:\n",
    "            t1_eq_val: Team 1 equipment value\n",
    "            t1_money_spent: Team 1 money spent this round\n",
    "            t1_rank: Team 1 HLTV ranking (lower is better)\n",
    "            t2_eq_val: Team 2 equipment value\n",
    "            t2_money_spent: Team 2 money spent this round\n",
    "            t2_rank: Team 2 HLTV ranking (lower is better)\n",
    "        \n",
    "        Returns:\n",
    "            float: Probability of Team 1 winning (0.0 to 1.0)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calculate differences\n",
    "        diff_eq_val = t1_eq_val - t2_eq_val\n",
    "        diff_money_spent = t1_money_spent - t2_money_spent\n",
    "        diff_rank = t2_rank - t1_rank  # Positive means T1 is stronger\n",
    "        \n",
    "        # Create feature vector (must match training order)\n",
    "        features = np.array([[\n",
    "            t1_eq_val,\n",
    "            t1_money_spent,\n",
    "            t1_rank,\n",
    "            t2_eq_val,\n",
    "            t2_money_spent,\n",
    "            t2_rank,\n",
    "            diff_eq_val,\n",
    "            diff_money_spent,\n",
    "            diff_rank,\n",
    "            diff_eq_val * diff_rank,  # eq_x_rank interaction\n",
    "            diff_money_spent * diff_rank,  # spending_x_rank interaction\n",
    "            t1_eq_val / (t2_eq_val + 1)  # eq_ratio\n",
    "        ]])\n",
    "        \n",
    "        # Scale features if using Logistic Regression\n",
    "        if self.scaler is not None:\n",
    "            features = self.scaler.transform(features)\n",
    "        \n",
    "        # Get probability\n",
    "        probability = self.model.predict_proba(features)[0, 1]\n",
    "        \n",
    "        return probability\n",
    "    \n",
    "    def predict_batch(self, team_states):\n",
    "        \"\"\"\n",
    "        Predict win probabilities for multiple scenarios\n",
    "        \n",
    "        Args:\n",
    "            team_states: list of dicts with keys:\n",
    "                        't1_eq_val', 't1_money_spent', 't1_rank',\n",
    "                        't2_eq_val', 't2_money_spent', 't2_rank'\n",
    "        \n",
    "        Returns:\n",
    "            list: Win probabilities for each scenario\n",
    "        \"\"\"\n",
    "        \n",
    "        probabilities = []\n",
    "        for state in team_states:\n",
    "            prob = self.predict_win_probability(\n",
    "                state['t1_eq_val'],\n",
    "                state['t1_money_spent'],\n",
    "                state['t1_rank'],\n",
    "                state['t2_eq_val'],\n",
    "                state['t2_money_spent'],\n",
    "                state['t2_rank']\n",
    "            )\n",
    "            probabilities.append(prob)\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLE IN ABM\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize calculator\n",
    "calculator = CSGOWinProbabilityCalculator()\n",
    "\n",
    "# Example: Calculate win probability for a round\n",
    "team1_eq = 20000  # Team 1 total equipment value\n",
    "team1_spent = 18500  # Team 1 money spent\n",
    "team1_rank = 5  # Team 1 is rank 5 (strong)\n",
    "\n",
    "team2_eq = 16000  # Team 2 total equipment value\n",
    "team2_spent = 15000  # Team 2 money spent\n",
    "team2_rank = 12  # Team 2 is rank 12 (weaker)\n",
    "\n",
    "win_prob = calculator.predict_win_probability(\n",
    "    t1_eq_val=team1_eq,\n",
    "    t1_money_spent=team1_spent,\n",
    "    t1_rank=team1_rank,\n",
    "    t2_eq_val=team2_eq,\n",
    "    t2_money_spent=team2_spent,\n",
    "    t2_rank=team2_rank\n",
    ")\n",
    "\n",
    "print(f\"Team 1 Win Probability: {{win_prob:.3f}}\")\n",
    "\n",
    "# Use in ABM simulation\n",
    "import random\n",
    "\n",
    "def simulate_round_outcome(calculator, t1_state, t2_state):\n",
    "    \"\"\"Simulate a round outcome using the trained model\"\"\"\n",
    "    \n",
    "    win_prob = calculator.predict_win_probability(\n",
    "        t1_state['eq_val'],\n",
    "        t1_state['money_spent'],\n",
    "        t1_state['rank'],\n",
    "        t2_state['eq_val'],\n",
    "        t2_state['money_spent'],\n",
    "        t2_state['rank']\n",
    "    )\n",
    "    \n",
    "    # Simulate outcome based on probability\n",
    "    team1_wins = random.random() < win_prob\n",
    "    \n",
    "    return team1_wins, win_prob\n",
    "\n",
    "# Example ABM round simulation\n",
    "t1_state = {{'eq_val': 20000, 'money_spent': 18500, 'rank': 5}}\n",
    "t2_state = {{'eq_val': 16000, 'money_spent': 15000, 'rank': 12}}\n",
    "\n",
    "outcome, probability = simulate_round_outcome(calculator, t1_state, t2_state)\n",
    "print(f\"Round outcome: {{'Team 1 Wins' if outcome else 'Team 2 Wins'}} (p={{probability:.3f}})\")\n",
    "'''\n",
    "    \n",
    "    # Save integration code\n",
    "    code_filename = f\"csgo_abm_integration_{export_timestamp}.py\"\n",
    "    with open(code_filename, 'w') as f:\n",
    "        f.write(integration_code)\n",
    "    \n",
    "    print(f\"‚úÖ Integration code exported to: {code_filename}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ EXPORT COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nüì¶ Exported Files:\")\n",
    "    print(f\"  1. {pickle_filename} - Complete model package\")\n",
    "    print(f\"  2. {json_filename} - Model metadata\")\n",
    "    print(f\"  3. {code_filename} - ABM integration code\")\n",
    "    \n",
    "    print(f\"\\nüìä Model Performance Summary:\")\n",
    "    print(f\"  Model Type: {best_model_name}\")\n",
    "    print(f\"  Test ROC-AUC: {model_results[best_model_name]['test_roc_auc']:.4f}\")\n",
    "    print(f\"  Test Accuracy: {model_results[best_model_name]['test_accuracy']:.4f}\")\n",
    "    print(f\"  Test F1-Score: {model_results[best_model_name]['test_f1']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Ready for ABM Integration!\")\n",
    "    print(f\"  - Load model using pickle\")\n",
    "    print(f\"  - Use CSGOWinProbabilityCalculator class\")\n",
    "    print(f\"  - Input: equipment values, spending, HLTV rankings\")\n",
    "    print(f\"  - Output: win probability (0.0 to 1.0)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No model available for export\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
